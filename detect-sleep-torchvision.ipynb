{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#查看包的版本\n",
    "#\"torchvision\" 是一个常用于处理图像和视频的 Python 包，它是 PyTorch 的一个扩展库。Torchvision 提供了以下三个主要功能：\n",
    "#torchvision.datasets: 提供了常用的数据集加载，比如 ImageNet、CIFAR10、MNIST 等，并且支持数据转换。\n",
    "#torchvision.models: 提供了深度学习中常用的模型结构以及预训练模型，比如 AlexNet、VGG、ResNet、DenseNet 等。\n",
    "#torchvision.transforms: 提供了常用的图像处理操作，比如裁剪、旋转、翻转、归一化等。\n",
    "\n",
    "!pip show torchvision"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:11:30.031749Z",
     "iopub.execute_input": "2023-11-10T19:11:30.032029Z",
     "iopub.status.idle": "2023-11-10T19:11:41.908355Z",
     "shell.execute_reply.started": "2023-11-10T19:11:30.032003Z",
     "shell.execute_reply": "2023-11-10T19:11:41.907440Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Name: torchvision\nVersion: 0.15.1\nSummary: image and video datasets and models for torch deep learning\nHome-page: https://github.com/pytorch/vision\nAuthor: PyTorch Core Team\nAuthor-email: soumith@pytorch.org\nLicense: BSD\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: numpy, pillow, requests, torch\nRequired-by: easyocr, fastai, timm\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:11:47.005596Z",
     "iopub.execute_input": "2023-11-10T19:11:47.006370Z",
     "iopub.status.idle": "2023-11-10T19:11:50.579991Z",
     "shell.execute_reply.started": "2023-11-10T19:11:47.006332Z",
     "shell.execute_reply": "2023-11-10T19:11:50.579034Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n  warnings.warn(_BETA_TRANSFORMS_WARNING)\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Object detection data prep"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "working_folder = Path(\"/kaggle/working/\")\n",
    "images_folder = working_folder/\"images\"\n",
    "images_folder.mkdir()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:11:56.950869Z",
     "iopub.execute_input": "2023-11-10T19:11:56.951210Z",
     "iopub.status.idle": "2023-11-10T19:11:57.651728Z",
     "shell.execute_reply.started": "2023-11-10T19:11:56.951183Z",
     "shell.execute_reply": "2023-11-10T19:11:57.650461Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileExistsError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m working_folder \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/kaggle/working/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m images_folder \u001B[38;5;241m=\u001B[39m working_folder\u001B[38;5;241m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mimages_folder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmkdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/pathlib.py:1175\u001B[0m, in \u001B[0;36mPath.mkdir\u001B[0;34m(self, mode, parents, exist_ok)\u001B[0m\n\u001B[1;32m   1171\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1172\u001B[0m \u001B[38;5;124;03mCreate a new directory at this given path.\u001B[39;00m\n\u001B[1;32m   1173\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1174\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1175\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_accessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmkdir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1176\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[1;32m   1177\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parents \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparent \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m:\n",
      "\u001B[0;31mFileExistsError\u001B[0m: [Errno 17] File exists: '/kaggle/working/images'"
     ],
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/kaggle/working/images'",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data_folder = Path(\"/kaggle/input/child-mind-institute-detect-sleep-states\")\n",
    "!ls {data_folder}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:12:00.615766Z",
     "iopub.execute_input": "2023-11-10T19:12:00.616548Z",
     "iopub.status.idle": "2023-11-10T19:12:01.571574Z",
     "shell.execute_reply.started": "2023-11-10T19:12:00.616511Z",
     "shell.execute_reply": "2023-11-10T19:12:01.570305Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "sample_submission.csv  train_events.csv\ntest_series.parquet    train_series.parquet\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#pandas 的 read_parquet 函数可以读取 Parquet 文件并将其转换为 DataFrame。\n",
    "test_series = pd.read_parquet(data_folder/\"test_series.parquet\")\n",
    "test_series"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:12:03.797360Z",
     "iopub.execute_input": "2023-11-10T19:12:03.798335Z",
     "iopub.status.idle": "2023-11-10T19:12:04.017228Z",
     "shell.execute_reply.started": "2023-11-10T19:12:03.798288Z",
     "shell.execute_reply": "2023-11-10T19:12:04.016170Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        series_id  step                 timestamp     anglez    enmo\n0    038441c925bb     0  2018-08-14T15:30:00-0400   2.636700  0.0217\n1    038441c925bb     1  2018-08-14T15:30:05-0400   2.636800  0.0215\n2    038441c925bb     2  2018-08-14T15:30:10-0400   2.637000  0.0216\n3    038441c925bb     3  2018-08-14T15:30:15-0400   2.636800  0.0213\n4    038441c925bb     4  2018-08-14T15:30:20-0400   2.636800  0.0215\n..            ...   ...                       ...        ...     ...\n445  0402a003dae9   145  2018-12-18T12:57:05-0500 -59.696899  0.0601\n446  0402a003dae9   146  2018-12-18T12:57:10-0500 -35.656601  0.0427\n447  0402a003dae9   147  2018-12-18T12:57:15-0500 -21.582399  0.0309\n448  0402a003dae9   148  2018-12-18T12:57:20-0500 -42.616001  0.0328\n449  0402a003dae9   149  2018-12-18T12:57:25-0500   7.029900  0.0081\n\n[450 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>series_id</th>\n      <th>step</th>\n      <th>timestamp</th>\n      <th>anglez</th>\n      <th>enmo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>038441c925bb</td>\n      <td>0</td>\n      <td>2018-08-14T15:30:00-0400</td>\n      <td>2.636700</td>\n      <td>0.0217</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>038441c925bb</td>\n      <td>1</td>\n      <td>2018-08-14T15:30:05-0400</td>\n      <td>2.636800</td>\n      <td>0.0215</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>038441c925bb</td>\n      <td>2</td>\n      <td>2018-08-14T15:30:10-0400</td>\n      <td>2.637000</td>\n      <td>0.0216</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>038441c925bb</td>\n      <td>3</td>\n      <td>2018-08-14T15:30:15-0400</td>\n      <td>2.636800</td>\n      <td>0.0213</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>038441c925bb</td>\n      <td>4</td>\n      <td>2018-08-14T15:30:20-0400</td>\n      <td>2.636800</td>\n      <td>0.0215</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>445</th>\n      <td>0402a003dae9</td>\n      <td>145</td>\n      <td>2018-12-18T12:57:05-0500</td>\n      <td>-59.696899</td>\n      <td>0.0601</td>\n    </tr>\n    <tr>\n      <th>446</th>\n      <td>0402a003dae9</td>\n      <td>146</td>\n      <td>2018-12-18T12:57:10-0500</td>\n      <td>-35.656601</td>\n      <td>0.0427</td>\n    </tr>\n    <tr>\n      <th>447</th>\n      <td>0402a003dae9</td>\n      <td>147</td>\n      <td>2018-12-18T12:57:15-0500</td>\n      <td>-21.582399</td>\n      <td>0.0309</td>\n    </tr>\n    <tr>\n      <th>448</th>\n      <td>0402a003dae9</td>\n      <td>148</td>\n      <td>2018-12-18T12:57:20-0500</td>\n      <td>-42.616001</td>\n      <td>0.0328</td>\n    </tr>\n    <tr>\n      <th>449</th>\n      <td>0402a003dae9</td>\n      <td>149</td>\n      <td>2018-12-18T12:57:25-0500</td>\n      <td>7.029900</td>\n      <td>0.0081</td>\n    </tr>\n  </tbody>\n</table>\n<p>450 rows × 5 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#找出 enmo 值较大的行，因为这些在分析中具有特殊的意义。\n",
    "test_series['large_enmo'] = test_series['enmo'] > 0.1506"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:12:07.441386Z",
     "iopub.execute_input": "2023-11-10T19:12:07.442064Z",
     "iopub.status.idle": "2023-11-10T19:12:07.447298Z",
     "shell.execute_reply.started": "2023-11-10T19:12:07.442030Z",
     "shell.execute_reply": "2023-11-10T19:12:07.446379Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#唯一值的数量\n",
    "series_ids = test_series['series_id'].unique()\n",
    "len(series_ids)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:12:09.725348Z",
     "iopub.execute_input": "2023-11-10T19:12:09.726382Z",
     "iopub.status.idle": "2023-11-10T19:12:09.735050Z",
     "shell.execute_reply.started": "2023-11-10T19:12:09.726338Z",
     "shell.execute_reply": "2023-11-10T19:12:09.733768Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "3"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "window_properties = []#创建一个空列表，存储每个时间窗口的属性。\n",
    "for i in range(len(series_ids)):#遍历 series_ids 中的每个唯一的 series_id\n",
    "    print(\"i:\", i)\n",
    "    print(\"series_id:\", series_ids[i])\n",
    "    #并将这些行组成一个新的 DataFrame，然后重置这个 DataFrame 的索引。\n",
    "    series = test_series.loc[test_series['series_id'] == series_ids[i]].reset_index(drop=True)\n",
    "    series['color'] = [\"blue\" if large_enmo else \"green\" for large_enmo in series['large_enmo']]\n",
    "    series['timestamp'] = pd.to_datetime(series['timestamp'])\n",
    "    #在处理跨越多个时区的数据，就可以将所有的时间都转换为 UTC，这样就可以避免由于时区转换而导致的问题。\n",
    "    series['timestamp_utc'] = series['timestamp'].map(lambda timestamp: timestamp.astimezone(timezone.utc))\n",
    "    #将 series DataFrame 中的 anglez 列中的每个角度值转换为弧度值，并将结果保存在新的 anglez_radians 列中。转换公式是角度值乘以 π/180。\n",
    "    series['anglez_radians'] = (np.pi / 180) * series['anglez']\n",
    "    series['cos_anglez'] = np.cos(series['anglez_radians'])\n",
    "    series['enmo'] = np.clip(series['enmo'], 0, 1)\n",
    "    min_date_utc = series['timestamp_utc'].dt.date.min()\n",
    "    max_date_utc = series['timestamp_utc'].dt.date.max()\n",
    "    series_24_hour_windows = {}    \n",
    "    upper_bound = datetime(year=min_date_utc.year, month=min_date_utc.month, day=min_date_utc.day, hour=17, tzinfo=timezone.utc)\n",
    "    lower_bound = upper_bound + timedelta(hours=-24) # 5pm UTC on the previous day.\n",
    "    while lower_bound < series['timestamp_utc'].max():\n",
    "        window_df = series.loc[(series['timestamp_utc'] >= lower_bound) & (series['timestamp_utc'] < upper_bound)].reset_index(drop=True)\n",
    "        if len(window_df) > 0:\n",
    "            series_24_hour_windows[upper_bound.isoformat()[:-6]] = window_df\n",
    "        upper_bound += timedelta(hours=24)\n",
    "        lower_bound += timedelta(hours=24)\n",
    "    \n",
    "    windows = list(series_24_hour_windows.keys())\n",
    "    num_steps_cumulative = 0\n",
    "    for window_idx, window in enumerate(windows):        \n",
    "        fig = plt.figure(figsize=(14.4, 4))  # (width, height) in inches\n",
    "        plt.plot(series_24_hour_windows[window]['timestamp_utc'], series_24_hour_windows[window]['cos_anglez'], color=\"red\")\n",
    "        plt.scatter(series_24_hour_windows[window]['timestamp_utc'], series_24_hour_windows[window]['enmo'], color=series_24_hour_windows[window]['color'], s=1)\n",
    "        ax = plt.gca()\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.margins(0, 0)\n",
    "        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "        plt.savefig(images_folder/f\"{series_ids[i]}_{window}.jpg\", bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "\n",
    "        min_ts_window = series_24_hour_windows[window]['timestamp_utc'].min()\n",
    "        max_ts_window = series_24_hour_windows[window]['timestamp_utc'].max()\n",
    "        num_steps_window = (max_ts_window - min_ts_window).total_seconds() / 5 + 1\n",
    "        num_steps_cumulative += num_steps_window\n",
    "        window_properties.append({\n",
    "            'series_id': series_ids[i], \n",
    "            'image_name': f\"{series_ids[i]}_{window}.jpg\", \n",
    "            'idx_in_series': window_idx, \n",
    "            'num_steps_window': num_steps_window, \n",
    "            'num_steps_cumulative': num_steps_cumulative\n",
    "        })\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:12:11.951029Z",
     "iopub.execute_input": "2023-11-10T19:12:11.951809Z",
     "iopub.status.idle": "2023-11-10T19:12:12.679660Z",
     "shell.execute_reply.started": "2023-11-10T19:12:11.951774Z",
     "shell.execute_reply": "2023-11-10T19:12:12.678588Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "i: 0\nseries_id: 038441c925bb\ni: 1\nseries_id: 03d92c9f6f8a\ni: 2\nseries_id: 0402a003dae9\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "window_properties_df = pd.DataFrame(window_properties)\n",
    "window_properties_df"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "      series_id                            image_name  idx_in_series  \\\n0  038441c925bb  038441c925bb_2018-08-15T17:00:00.jpg              0   \n1  03d92c9f6f8a  03d92c9f6f8a_2018-05-31T17:00:00.jpg              0   \n2  0402a003dae9  0402a003dae9_2018-12-19T17:00:00.jpg              0   \n\n   num_steps_window  num_steps_cumulative  \n0             150.0                 150.0  \n1             150.0                 150.0  \n2             150.0                 150.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>series_id</th>\n      <th>image_name</th>\n      <th>idx_in_series</th>\n      <th>num_steps_window</th>\n      <th>num_steps_cumulative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>038441c925bb</td>\n      <td>038441c925bb_2018-08-15T17:00:00.jpg</td>\n      <td>0</td>\n      <td>150.0</td>\n      <td>150.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>03d92c9f6f8a</td>\n      <td>03d92c9f6f8a_2018-05-31T17:00:00.jpg</td>\n      <td>0</td>\n      <td>150.0</td>\n      <td>150.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0402a003dae9</td>\n      <td>0402a003dae9_2018-12-19T17:00:00.jpg</td>\n      <td>0</td>\n      <td>150.0</td>\n      <td>150.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transforms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def get_transforms():\n",
    "    transforms = []\n",
    "    transforms.append(T.ConvertDtype(torch.float))\n",
    "    return T.Compose(transforms)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:13:53.341650Z",
     "iopub.execute_input": "2023-11-10T19:13:53.342009Z",
     "iopub.status.idle": "2023-11-10T19:13:53.346714Z",
     "shell.execute_reply.started": "2023-11-10T19:13:53.341970Z",
     "shell.execute_reply": "2023-11-10T19:13:53.345832Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the <a href=\"https://pytorch.org/vision/0.15/generated/torchvision.transforms.v2.ConvertDtype.html\" target=\"_blank\">documentation</a>:\n",
    "\n",
    "> Convert input image or video to the given `dtype` and scale the values accordingly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "eval_transforms = get_transforms()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instantiating the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:10:52.748935Z",
     "iopub.status.idle": "2023-11-10T19:10:52.749394Z",
     "shell.execute_reply.started": "2023-11-10T19:10:52.749146Z",
     "shell.execute_reply": "2023-11-10T19:10:52.749169Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load an object detection model pre-trained on COCO:\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None, weights_backbone=None, max_size=1440)\n",
    "# Replace the classifier with a new one, that has num_classes which is user-defined:\n",
    "num_classes = 3 # 2 classes ('onset' & 'wakeup') + the 'background' class\n",
    "# Get the number of input features for the box classifier:\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# Replace the pre-trained box predictor head with a new one:\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:10:52.751214Z",
     "iopub.status.idle": "2023-11-10T19:10:52.751680Z",
     "shell.execute_reply.started": "2023-11-10T19:10:52.751446Z",
     "shell.execute_reply": "2023-11-10T19:10:52.751468Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "weights = torch.load(\"/kaggle/input/c-m-i-d-s-s-extra-data/m1_4.pth\", map_location=device)\n",
    "model.load_state_dict(weights)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:10:52.752710Z",
     "iopub.status.idle": "2023-11-10T19:10:52.753213Z",
     "shell.execute_reply.started": "2023-11-10T19:10:52.752945Z",
     "shell.execute_reply": "2023-11-10T19:10:52.752971Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.to(device);"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:10:52.754712Z",
     "iopub.status.idle": "2023-11-10T19:10:52.755276Z",
     "shell.execute_reply.started": "2023-11-10T19:10:52.754952Z",
     "shell.execute_reply": "2023-11-10T19:10:52.754976Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "thresh = 0.0 # Tune this.\n",
    "reverse_label_mapping = {1: \"onset\", 2: \"wakeup\"}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:10:52.756556Z",
     "iopub.status.idle": "2023-11-10T19:10:52.757010Z",
     "shell.execute_reply.started": "2023-11-10T19:10:52.756779Z",
     "shell.execute_reply": "2023-11-10T19:10:52.756802Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_submission_df():\n",
    "    all_test_preds = []\n",
    "    for i, series_id in enumerate(series_ids): # i is the index of the series.\n",
    "        series_rows = window_properties_df.loc[window_properties_df['series_id'] == series_id].reset_index(drop=True)\n",
    "        for j in range(len(series_rows)): # j is the index of the image (within the series).\n",
    "            image_name = series_rows['image_name'][j]\n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "            image = read_image(image_path)\n",
    "            with torch.no_grad():\n",
    "                x = eval_transforms(image)\n",
    "                x = x.to(device)\n",
    "                predictions = model([x])\n",
    "            pred = predictions[0]\n",
    "            \n",
    "            # Postprocessing: remove 'narrow' boxes as these are false positives.\n",
    "            pred_widths = pred['boxes'][:, 2] - pred['boxes'][:, 0]\n",
    "            pred['boxes'] = pred['boxes'][pred_widths > 10]\n",
    "            pred['labels'] = pred['labels'][pred_widths > 10]\n",
    "            pred['scores'] = pred['scores'][pred_widths > 10]\n",
    "            \n",
    "            pred['boxes'] = pred['boxes'][pred['scores'] > thresh]\n",
    "            pred['labels'] = pred['labels'][pred['scores'] > thresh]\n",
    "            pred['scores'] = pred['scores'][pred['scores'] > thresh]\n",
    "            \n",
    "            if len(pred['labels']) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                pred_x = (pred['boxes'][:, 0] + pred['boxes'][:, 2]) / 2\n",
    "                pred_labels = [reverse_label_mapping[l.item()] for l in pred['labels']]\n",
    "                num_steps_window = series_rows['num_steps_window'][j]\n",
    "                if j == 0:\n",
    "                    prev_num_steps_cumulative = 0\n",
    "                else:\n",
    "                    prev_num_steps_cumulative = series_rows['num_steps_cumulative'][j - 1]            \n",
    "                for k in range(len(pred_labels)): # k is the index of the bounding box (within the image).\n",
    "                    event = {}\n",
    "                    step_in_window = (pred_x[k] / 1440) * num_steps_window\n",
    "                    step_in_series = int(prev_num_steps_cumulative + step_in_window)\n",
    "                    event['series_id'] = series_id\n",
    "                    event['step'] = step_in_series\n",
    "                    event['event'] = pred_labels[k]\n",
    "                    event['score'] = pred['scores'][k].item()\n",
    "                    all_test_preds.append(event)          \n",
    "    if len(all_test_preds) > 0:\n",
    "        submission_df = pd.DataFrame(all_test_preds)\n",
    "        submission_df = submission_df.sort_values(by=['series_id', 'step']).reset_index(drop=True)\n",
    "        submission_df['row_id'] = np.arange(len(submission_df))\n",
    "        submission_df = submission_df[['row_id', 'series_id', 'step', 'event', 'score']]\n",
    "    else:\n",
    "        submission_df = pd.DataFrame({'row_id': [], 'series_id': [], 'step': [], 'event': [], 'score': []})\n",
    "    return submission_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:10:52.758160Z",
     "iopub.status.idle": "2023-11-10T19:10:52.758526Z",
     "shell.execute_reply.started": "2023-11-10T19:10:52.758331Z",
     "shell.execute_reply": "2023-11-10T19:10:52.758347Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission_df = get_submission_df()\n",
    "submission_df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:10:52.759823Z",
     "iopub.status.idle": "2023-11-10T19:10:52.760153Z",
     "shell.execute_reply.started": "2023-11-10T19:10:52.759991Z",
     "shell.execute_reply": "2023-11-10T19:10:52.760007Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -rf /kaggle/working/images"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:10:52.761512Z",
     "iopub.status.idle": "2023-11-10T19:10:52.761826Z",
     "shell.execute_reply.started": "2023-11-10T19:10:52.761665Z",
     "shell.execute_reply": "2023-11-10T19:10:52.761679Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-10T19:10:52.763162Z",
     "iopub.status.idle": "2023-11-10T19:10:52.763590Z",
     "shell.execute_reply.started": "2023-11-10T19:10:52.763358Z",
     "shell.execute_reply": "2023-11-10T19:10:52.763399Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
